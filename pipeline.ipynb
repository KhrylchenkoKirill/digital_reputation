{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from utils.data import Data, TestData\n",
    "from utils.validation import MultiClassValidator\n",
    "from utils.models import CVModel\n",
    "from utils.text import Tokenizer\n",
    "\n",
    "data = Data('./data/train/')\n",
    "test_data = TestData('./data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tok = Tokenizer(min_df=3)\n",
    "data.X2_corpus = tok.transform(data.X2)\n",
    "test_data.X2_corpus = tok.transform(test_data.X2)\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "train_corpus = vect.fit_transform(data.X2_corpus.map(lambda x: ' '.join(map(str, x))))\n",
    "test_corpus = vect.transform(test_data.X2_corpus.map(lambda x: ' '.join(map(str, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, get_model, fit_model, n_splits=10, test_data=None, sparse=False, seed=19, verbose=False):\n",
    "    \n",
    "    val = MultiClassValidator(data.Y.values)\n",
    "    val_preds = []\n",
    "    if test_data is not None:\n",
    "        test_preds = []\n",
    "    scores = []\n",
    "    for target in range(5):\n",
    "        model = CVModel(get_model, fit_model)\n",
    "        score, preds = model.fit(\n",
    "            data.X2_corpus, data.Y[str(target + 1)],\n",
    "            n_splits=n_splits, seed=seed, validator=val,\n",
    "            sparse=sparse, cache=True, verbose=False\n",
    "        )\n",
    "        scores.append(score)\n",
    "        val_preds.append(preds)\n",
    "        if verbose:\n",
    "            print('target {}, score: {:.4f}'.format(target + 1, score))\n",
    "        if test_data is not None:\n",
    "            test_preds.append(model.predict(test_data.X2_corpus))\n",
    "    score = np.mean(scores)\n",
    "    if verbose:\n",
    "        print('macro, score: {:.4f}'.format(score))\n",
    "    val_preds = np.vstack(val_preds).T\n",
    "    \n",
    "    if test_data is None:\n",
    "        return score, val_preds\n",
    "    else:\n",
    "        test_preds = np.vstack(test_preds).T\n",
    "        return scores, val_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=250, n_iter=5, random_state=20)\n",
    "data.X2_corpus = svd.fit_transform(train_corpus)\n",
    "test_data.X2_corpus = svd.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_logreg(C=1., solver='lbfgs'):\n",
    "    def f():\n",
    "        return LogisticRegression(C=C, solver=solver, fit_intercept=True)\n",
    "    return f\n",
    "\n",
    "def fit_logreg():\n",
    "    def f(model, train_X, train_y, test_X, test_y):\n",
    "        model.fit(train_X, train_y)\n",
    "        score = roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])\n",
    "        return score\n",
    "    return f\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def get_knn(n_neighbors=5, metric='minkowski'):\n",
    "    def f():\n",
    "        return KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "    return f\n",
    "\n",
    "def fit_knn():\n",
    "    def f(model, train_X, train_y, test_X, test_y):\n",
    "        model.fit(train_X, train_y)\n",
    "        return roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])\n",
    "    return f\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def get_lgb(model_params):\n",
    "    def f():\n",
    "        return LGBMClassifier(**model_params)\n",
    "    return f\n",
    "\n",
    "def fit_lgb(train_params):\n",
    "    def f(model, train_X, train_y, test_X, test_y):\n",
    "        model.fit(train_X, train_y, eval_set=(test_X, test_y),  eval_metric='auc', verbose=False, **train_params)\n",
    "        return model.best_score_['valid_0']['auc']\n",
    "    return f\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 10000,\n",
    "    'num_leaves': 11,\n",
    "    'learning_rate': 0.05,\n",
    "    'metrics': None\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'early_stopping_rounds': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2_logreg\n",
      "target 1, score: 0.5687\n",
      "target 2, score: 0.6148\n",
      "target 3, score: 0.6107\n",
      "target 4, score: 0.6116\n",
      "target 5, score: 0.6311\n",
      "macro, score: 0.6074\n",
      "\n",
      "X2_knn\n",
      "target 1, score: 0.5800\n",
      "target 2, score: 0.6156\n",
      "target 3, score: 0.6123\n",
      "target 4, score: 0.6069\n",
      "target 5, score: 0.6229\n",
      "macro, score: 0.6075\n",
      "\n",
      "X2_gbm\n",
      "target 1, score: 0.5724\n",
      "target 2, score: 0.6213\n",
      "target 3, score: 0.6063\n",
      "target 4, score: 0.6071\n",
      "target 5, score: 0.6319\n",
      "macro, score: 0.6078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('X2_logreg', get_logreg(C=0.1, solver='lbfgs'), fit_logreg()),\n",
    "    ('X2_knn', get_knn(n_neighbors=300, metric='cosine'), fit_knn()),\n",
    "    ('X2_gbm', get_lgb(lgb_params), fit_lgb(train_params))\n",
    "]\n",
    "\n",
    "\n",
    "val_preds = dict()\n",
    "test_preds = dict()\n",
    "scores = dict()\n",
    "\n",
    "for model_name, get_model, fit_model in models:\n",
    "    \n",
    "    print(model_name)\n",
    "    scores[model_name], val_preds[model_name], test_preds[model_name] = train(\n",
    "        data, get_model, fit_model, sparse=True,\n",
    "        n_splits=10, test_data=test_data, seed=19, verbose=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(data.X2_corpus, index=data.X1.index)\n",
    "tmp.rename(columns=lambda x: 'X2_' + str(x), inplace=True)\n",
    "data.X2_corpus = tmp.merge(data.X1, left_index=True, right_index=True, how='left')\n",
    "\n",
    "tmp = pd.DataFrame(test_data.X2_corpus, index=test_data.X1.index)\n",
    "tmp.rename(columns=lambda x: 'X2_' + str(x), inplace=True)\n",
    "test_data.X2_corpus = tmp.merge(test_data.X1, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_X2_gbm\n",
      "target 1, score: 0.6162\n",
      "target 2, score: 0.6278\n",
      "target 3, score: 0.6352\n",
      "target 4, score: 0.6154\n",
      "target 5, score: 0.6305\n",
      "macro, score: 0.6250\n",
      "\n",
      "X1_X2_cat\n",
      "target 1, score: 0.6062\n",
      "target 2, score: 0.6291\n",
      "target 3, score: 0.6323\n",
      "target 4, score: 0.6199\n",
      "target 5, score: 0.6263\n",
      "macro, score: 0.6227\n",
      "\n",
      "X1_X2_xgb\n",
      "target 1, score: 0.5819\n",
      "target 2, score: 0.6197\n",
      "target 3, score: 0.6134\n",
      "target 4, score: 0.5941\n",
      "target 5, score: 0.6133\n",
      "macro, score: 0.6045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def get_cat(model_params):\n",
    "    def f():\n",
    "        return CatBoostClassifier(**model_params)\n",
    "    return f\n",
    "\n",
    "def fit_cat(train_params):\n",
    "    def f(model, train_X, train_y, test_X, test_y):\n",
    "        model.fit(train_X, train_y, eval_set=[(test_X, test_y)], **train_params)\n",
    "        return model.best_score_['validation_0']['AUC']\n",
    "    return f\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 10000,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 5,\n",
    "    'eval_metric': 'AUC'\n",
    "}\n",
    "\n",
    "cat_train_params = {\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def get_xgb(model_params):\n",
    "    def f():\n",
    "        return XGBClassifier(**model_params)\n",
    "    return f\n",
    "\n",
    "def fit_xgb(train_params):\n",
    "    def f(model, train_X, train_y, test_X, test_y):\n",
    "        model.fit(train_X, train_y, eval_set=[(test_X, test_y)], **train_params, verbose=False)\n",
    "        \n",
    "        return roc_auc_score(test_y, model.predict_proba(test_X)[:, 1])\n",
    "    return f\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 4,\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.05\n",
    "}\n",
    "\n",
    "new_models = [\n",
    "    ('X1_X2_gbm', get_lgb(lgb_params), fit_lgb(train_params)),\n",
    "    ('X1_X2_cat', get_cat(cat_params), fit_cat(cat_train_params)),\n",
    "    ('X1_X2_xgb', get_xgb(xgb_params), fit_xgb(train_params))\n",
    "]\n",
    "\n",
    "for model_name, get_model, fit_model in new_models:\n",
    "    \n",
    "    print(model_name)\n",
    "    scores[model_name], val_preds[model_name], test_preds[model_name] = train(\n",
    "        data, get_model, fit_model, sparse=False,\n",
    "        n_splits=10, test_data=test_data, seed=19, verbose=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/digital_reputation_challenge_sample_submit.csv')\n",
    "sub[['1', '2', '3', '4', '5']] = np.mean([test_preds[model] for model in test_preds], axis=0)\n",
    "sub.to_csv('./data/mean_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6219113268464954\n",
      "0.6341430030603811\n",
      "0.6254590733471446\n",
      "0.6158166720607764\n",
      "0.6417339679944092\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "tau = 30\n",
    "\n",
    "final_test_preds = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    weights = softmax([tau * scores[model][i] for model in list(scores)])\n",
    "    print(\n",
    "        roc_auc_score(\n",
    "            data.Y[str(i + 1)],\n",
    "            np.sum([weight * val_preds[model][:, i] for weight, model in zip(weights, list(scores))], axis=0)\n",
    "        )\n",
    "    )\n",
    "    final_test_preds.append(\n",
    "        np.sum([weight * test_preds[model][:, i] for weight, model in zip(weights, list(scores))], axis=0)\n",
    "    )\n",
    "final_test_preds = np.array(final_test_preds).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[['1', '2', '3', '4', '5']] = final_test_preds\n",
    "sub.to_csv('./data/tau_30_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
